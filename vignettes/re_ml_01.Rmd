---
title: "Report exercise chapter 10"
author: "Robbert Kouwenhoven"
date: "2025-04-22"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages,  include = FALSE}
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(caret)
library(recipes)

# Load helper functions from ../R
source("../R/eval_model.R")
```


```{r loading useful data}
daily_fluxes <- read_csv("../raw_data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  
  
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))

daily_fluxes
 
```

```{r data cleaning}
# no long tail, therefore no further target engineering
daily_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()
```

```{r Preprocessing and fitting}
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(recipes::all_predictors()) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```

```{r evaluate and visualize linear regression}
# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

```{r Evaluate and visualize KNN model}
# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

Question 1: Comparison of the linear regression and KNN models

1.1 Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?
-> This is due to overfitting:
KNN is a low-bias, high-variance model. It adapts flexible to the training data, which results in a higher R² and lower RMSE on the training set (0.72 and 1.37).However, this flexibility also means that it doesn't generalize as well to unseen data, which is why the test set performance drops more notably (R² = 0.64, RMSE = 1.54). On the other side, linear regression is a high-bias, low-variance model. It's simpler and underfits the training data slightly, but its performance on test data remains close to that on training data.

1.2 Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?
-> KNN captures more complex, non-linear relationships in the data that the linear model cannot. Even after some performance degradation, it generalizes better than the linear model in this particular dataset. In other words, the linear regression model is underfitting.

1.3 How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?
-> linear regression: high bias, low variance, a little too simple but stables across data
KNN: low bias, high variance, more flexible but also prone to overfitting. With our chosen k = 8, it tries to find a balance but still shows some variance sensitivity.

```{r removing NaNs to allow temporal plotting}
daily_fluxes_test_clean <- daily_fluxes_test %>%
  filter(if_all(everything(), ~ !is.na(.)))
```


```{r Temporal plot}
plot_df <- bind_rows(
  daily_fluxes_test_clean %>%
    select(TIMESTAMP, GPP_obs = GPP_NT_VUT_REF) %>%
    mutate(model = "Observed"),
  data.frame(TIMESTAMP = daily_fluxes_test_clean$TIMESTAMP,
             GPP_obs = predict(mod_lm, newdata = daily_fluxes_test_clean),
             model = "Linear Regression"),
  data.frame(TIMESTAMP = daily_fluxes_test_clean$TIMESTAMP,
             GPP_obs = predict(mod_knn, newdata = daily_fluxes_test_clean),
             model = "KNN")
)

# Plot
ggplot(plot_df, aes(x = TIMESTAMP, y = GPP_obs, color = model)) +
  geom_line(alpha = 0.8) +
  labs(title = "Temporal variation of observed and modelled GPP",
       x = "Date", y = "GPP") +
  theme_minimal()
```

Question 2: the role of k

2.1 state a hypothesis for how the k and the MAE evaluated on the test and on the training set would change for k approaching 1 and for k approaching N.
-> if k=1, the nearest neighbour becomes the datapoint itself. This leads to a training error (MAE) of 0 and an R^2 of 1 for the training data. However, the test error will increase, due to overfitting. Also, this model will be very sensitive to noise in the training data
if k approaches N, the prediction becomes the average of (almost) all training data. Therefore the model will underfit, leading to a high bias and a high MAE for both training and testing data. R^2 will hence be low.

```{r function to loop over k}
get_mae_k <- function(k_val, pp, train_data, test_data) {
  mod_k <- caret::train(
    pp,
    data = train_data |> drop_na(),
    method = "knn",
    trControl = caret::trainControl(method = "none"),
    tuneGrid = data.frame(k = k_val)
  )
  
  metrics <- eval_model(mod_k, train_data, test_data, return_metrics = TRUE)
  mae_test <- metrics$test %>%
    dplyr::filter(.metric == "mae") %>%
    dplyr::pull(.estimate)
  
  return(mae_test)
}

```

```{r Loop over k values and plot}
k_values <- seq(1, 50, by = 2)
mae_results <- data.frame(
  k = k_values,
  MAE = sapply(k_values, get_mae_k, 
               pp = pp, 
               train_data = daily_fluxes_train, 
               test_data = daily_fluxes_test)
)

ggplot(mae_results, aes(x = k, y = MAE)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(color = "darkred") +
  labs(title = "MAE vs. k in KNN",
       x = "Number of Neighbors (k)",
       y = "Test MAE") +
  theme_minimal()
```

2.2 Describe how a “region” of overfitting and underfitting can be determined in your visualisation.
->In the overfitting region (low k values), the model becomes too flexible, essentially memorizing the training data. The test MAE is relatively high, because the model fails to generalize well to unseen data — it reacts too sensitively to noise or local fluctuations.
In the underfitting region (high k values), the model becomes too simple, averaging over many neighbors. It cannot capture the complexity of the relationship in the data, leading to a high bias.

```{r find optimal k}
best_k <- mae_results$k[which.min(mae_results$MAE)]
cat("Optimal k:", best_k)

```
Purely based on minimizing the MAE, the optimal k of this model would be k=19. 
